{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "from os import path\n",
    "from pandas import HDFStore\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from utils import get_callbacks,get_callbacks_t,windowed_dataset,plot_report,build_mdpi,split_train_test_val\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "EPOCHS = 1000\n",
    "INITIAL_EXAMPLES = 50\n",
    "FEATURES = 4096\n",
    "window_size = 128\n",
    "shuffle_buffer = 40000\n",
    "coeff = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "coeff = 17\n",
    "\n",
    "#load source origin hdf5 file\n",
    "root_path = \"Z:/elekin\"\n",
    "rdo_root_path = path.join(root_path,\"02-RESULTADOS/03-HANDWRITTING\")\n",
    "h5file = path.join(rdo_root_path, \"00-OUTPUT/archimedean-\")\n",
    "h5filename = h5file + str(coeff) + \".h5\"\n",
    "hdf = HDFStore(h5filename)\n",
    "\n",
    "raw_df = hdf['results/residues/rd'].T\n",
    "#normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(raw_df))\n",
    "#get labels\n",
    "scaled_df['labels'] = hdf.get('source/labels').values\n",
    "#shuffle dataset\n",
    "raw_dataset = scaled_df.sample(frac=1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "\n",
    "#preprocess labels to get the right data type for training\n",
    "labels = raw_dataset[\"labels\"].values.astype('int8')\n",
    "timeseries = raw_dataset.loc[:, raw_dataset.columns != 'labels'].values.astype(\"float32\")\n",
    "\n",
    "#data augmentation, build features windows\n",
    "for i,features in enumerate(timeseries):\n",
    "    new = windowed_dataset(features,window_size,window_size,labels[i])\n",
    "    if i>0:\n",
    "        dataset = tf.data.Dataset.concatenate(dataset,new)\n",
    "    else:\n",
    "        dataset = new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = int(len(list(iter(dataset.cache()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Very important to guarantee good shuffle data. Get some label values to review shuffle effect\n",
    "for x,y in dataset.take(15):\n",
    "    #print(\"x= \",x)\n",
    "    print(\"y= \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets splits\n",
    "train, val, test = split_train_test_val(dataset, DATASET_SIZE, train_ratio=0.7, \n",
    "                                                                val_ratio=0.15, test_ratio=0.15, shuffle_buffer=shuffle_buffer)\n",
    "train_dataset = train.batch(16).prefetch(1)\n",
    "val_dataset = val.batch(8).prefetch(1)\n",
    "test_dataset = test.batch(8).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138915, 29767, 29767)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to validate training splits execute cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(iter(train_dataset))),len(list(iter(val_dataset))),len(list(iter(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important to guarantee good shuffle data. Get some label values to review shuffle effect\n",
    "for x,y in train_dataset.take(2):\n",
    "    #print(\"x= \",x)\n",
    "    print(\"y= \",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Neural Network architecture and compile\n",
    "mdpi = build_mdpi(window_size)# build cnn1d architecture\n",
    "#mdpi.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-1, momentum=0.9),\n",
    "mdpi.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=7e-8, epsilon=None, decay=1e-4, beta_1=0.9, beta_2=0.99, amsgrad=False),\n",
    "                          loss='binary_crossentropy',metrics=['accuracy'])#compile\n",
    "#mdpi.summary()#review the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, accuracy:0.4966,  loss:0.7782,  val_accuracy:0.4781,  val_loss:0.7202,  \n",
      "..........\n",
      "Epoch: 10, accuracy:0.5006,  loss:0.7709,  val_accuracy:0.4747,  val_loss:0.7205,  \n",
      "..........\n",
      "Epoch: 20, accuracy:0.5019,  loss:0.7672,  val_accuracy:0.4769,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 30, accuracy:0.4992,  loss:0.7687,  val_accuracy:0.4780,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 40, accuracy:0.5011,  loss:0.7678,  val_accuracy:0.4779,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 50, accuracy:0.5009,  loss:0.7674,  val_accuracy:0.4783,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 60, accuracy:0.5008,  loss:0.7664,  val_accuracy:0.4782,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 70, accuracy:0.4999,  loss:0.7676,  val_accuracy:0.4781,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 80, accuracy:0.4991,  loss:0.7676,  val_accuracy:0.4782,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 90, accuracy:0.5004,  loss:0.7679,  val_accuracy:0.4785,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 100, accuracy:0.4989,  loss:0.7691,  val_accuracy:0.4786,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 110, accuracy:0.5001,  loss:0.7677,  val_accuracy:0.4786,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 120, accuracy:0.5011,  loss:0.7656,  val_accuracy:0.4785,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 130, accuracy:0.4992,  loss:0.7675,  val_accuracy:0.4786,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 140, accuracy:0.5032,  loss:0.7662,  val_accuracy:0.4784,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 150, accuracy:0.5005,  loss:0.7669,  val_accuracy:0.4786,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 160, accuracy:0.5008,  loss:0.7665,  val_accuracy:0.4786,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 170, accuracy:0.5009,  loss:0.7678,  val_accuracy:0.4785,  val_loss:0.7206,  \n",
      "..........\n",
      "Epoch: 180, accuracy:0.5036,  loss:0.7656,  val_accuracy:0.4784,  val_loss:0.7206,  \n",
      ".........."
     ]
    }
   ],
   "source": [
    "## Training\n",
    "tf.keras.backend.clear_session()# clean previous training cached data\n",
    "histories = {}\n",
    "#histories['mdpi'] = mdpi.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_callbacks('mdpi',10), verbose=0, use_multiprocessing=True,)\n",
    "histories['mdpi/local'] = mdpi.fit(train_dataset.cache(), validation_data=val_dataset.cache(), epochs=EPOCHS, callbacks=get_callbacks_t('mdpi/local',10), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation: plot training & validation accuracy values\n",
    "plot_report(histories['mdpi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = mdpi.evaluate(test_dataset)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
