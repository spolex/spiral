{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/elekin/data/results/handwriting/archimedean-17.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/source/dataset',\n",
       " '/source/labels',\n",
       " '/results/residues/features',\n",
       " '/results/residues/rd',\n",
       " '/results/radius/features',\n",
       " '/results/radius/r']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "from os import path\n",
    "from pandas import HDFStore\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "from utils import load_residues_dataset, split_train_test, windowed_dataset, split_train_test_val\n",
    "from config import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "window_size=round(4096/4)\n",
    "\n",
    "HDFStore=pd.HDFStore\n",
    "hdf = HDFStore(h5filename)\n",
    "h5filename=h5filename\n",
    "print(h5filename)\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "residues=hdf['/results/residues/rd']\n",
    "labels=hdf['/source/labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C01_11</th>\n",
       "      <th>C01_12</th>\n",
       "      <th>C02_11</th>\n",
       "      <th>C02_12</th>\n",
       "      <th>C03_11</th>\n",
       "      <th>C03_12</th>\n",
       "      <th>C04_11</th>\n",
       "      <th>C04_12</th>\n",
       "      <th>C05_11</th>\n",
       "      <th>C05_12</th>\n",
       "      <th>...</th>\n",
       "      <th>control2_01</th>\n",
       "      <th>control2_02</th>\n",
       "      <th>control3_01</th>\n",
       "      <th>control3_02</th>\n",
       "      <th>control4_01</th>\n",
       "      <th>control4_02</th>\n",
       "      <th>control6_01</th>\n",
       "      <th>control6_02</th>\n",
       "      <th>control7_01</th>\n",
       "      <th>control7_02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.608315</td>\n",
       "      <td>24.635253</td>\n",
       "      <td>29.937969</td>\n",
       "      <td>37.304354</td>\n",
       "      <td>22.586074</td>\n",
       "      <td>30.466801</td>\n",
       "      <td>33.041917</td>\n",
       "      <td>30.190067</td>\n",
       "      <td>38.624101</td>\n",
       "      <td>27.533153</td>\n",
       "      <td>...</td>\n",
       "      <td>59.642004</td>\n",
       "      <td>31.082821</td>\n",
       "      <td>64.339435</td>\n",
       "      <td>41.205316</td>\n",
       "      <td>99.378109</td>\n",
       "      <td>41.428295</td>\n",
       "      <td>53.405596</td>\n",
       "      <td>38.037733</td>\n",
       "      <td>40.292484</td>\n",
       "      <td>36.764331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.319607</td>\n",
       "      <td>13.461991</td>\n",
       "      <td>14.349422</td>\n",
       "      <td>27.618238</td>\n",
       "      <td>8.450348</td>\n",
       "      <td>12.839783</td>\n",
       "      <td>14.240844</td>\n",
       "      <td>13.321235</td>\n",
       "      <td>16.995977</td>\n",
       "      <td>11.984692</td>\n",
       "      <td>...</td>\n",
       "      <td>48.980261</td>\n",
       "      <td>17.824924</td>\n",
       "      <td>74.473012</td>\n",
       "      <td>42.341037</td>\n",
       "      <td>104.077622</td>\n",
       "      <td>19.682270</td>\n",
       "      <td>54.791133</td>\n",
       "      <td>13.718908</td>\n",
       "      <td>28.194947</td>\n",
       "      <td>28.651363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.465534</td>\n",
       "      <td>3.768582</td>\n",
       "      <td>7.828024</td>\n",
       "      <td>4.146649</td>\n",
       "      <td>6.015019</td>\n",
       "      <td>11.547645</td>\n",
       "      <td>7.665631</td>\n",
       "      <td>11.954557</td>\n",
       "      <td>8.443727</td>\n",
       "      <td>6.249616</td>\n",
       "      <td>...</td>\n",
       "      <td>12.247924</td>\n",
       "      <td>7.916369</td>\n",
       "      <td>11.581669</td>\n",
       "      <td>11.966519</td>\n",
       "      <td>20.971488</td>\n",
       "      <td>9.040138</td>\n",
       "      <td>12.812669</td>\n",
       "      <td>11.670371</td>\n",
       "      <td>11.576587</td>\n",
       "      <td>9.365653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.565670</td>\n",
       "      <td>15.123158</td>\n",
       "      <td>19.309250</td>\n",
       "      <td>16.097841</td>\n",
       "      <td>15.683448</td>\n",
       "      <td>20.843118</td>\n",
       "      <td>23.109047</td>\n",
       "      <td>18.031392</td>\n",
       "      <td>24.798825</td>\n",
       "      <td>18.709808</td>\n",
       "      <td>...</td>\n",
       "      <td>32.875997</td>\n",
       "      <td>18.972532</td>\n",
       "      <td>34.008533</td>\n",
       "      <td>23.128025</td>\n",
       "      <td>52.022034</td>\n",
       "      <td>24.981298</td>\n",
       "      <td>23.224120</td>\n",
       "      <td>29.212238</td>\n",
       "      <td>23.301624</td>\n",
       "      <td>22.219017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.413607</td>\n",
       "      <td>22.129791</td>\n",
       "      <td>26.309766</td>\n",
       "      <td>25.797659</td>\n",
       "      <td>22.756174</td>\n",
       "      <td>29.334299</td>\n",
       "      <td>32.367934</td>\n",
       "      <td>27.506147</td>\n",
       "      <td>37.733462</td>\n",
       "      <td>26.168477</td>\n",
       "      <td>...</td>\n",
       "      <td>47.727315</td>\n",
       "      <td>29.322149</td>\n",
       "      <td>49.708925</td>\n",
       "      <td>31.735030</td>\n",
       "      <td>73.901908</td>\n",
       "      <td>39.825411</td>\n",
       "      <td>38.909549</td>\n",
       "      <td>38.428227</td>\n",
       "      <td>33.536709</td>\n",
       "      <td>29.230624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.076994</td>\n",
       "      <td>31.842144</td>\n",
       "      <td>37.404580</td>\n",
       "      <td>53.354334</td>\n",
       "      <td>28.569266</td>\n",
       "      <td>35.338651</td>\n",
       "      <td>41.385368</td>\n",
       "      <td>37.609440</td>\n",
       "      <td>49.933597</td>\n",
       "      <td>36.194390</td>\n",
       "      <td>...</td>\n",
       "      <td>65.736838</td>\n",
       "      <td>37.287423</td>\n",
       "      <td>63.184969</td>\n",
       "      <td>46.008286</td>\n",
       "      <td>94.935978</td>\n",
       "      <td>55.150159</td>\n",
       "      <td>54.394438</td>\n",
       "      <td>47.324724</td>\n",
       "      <td>46.853046</td>\n",
       "      <td>40.857373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63.479527</td>\n",
       "      <td>74.167295</td>\n",
       "      <td>88.395436</td>\n",
       "      <td>115.503629</td>\n",
       "      <td>61.589324</td>\n",
       "      <td>71.039395</td>\n",
       "      <td>72.117287</td>\n",
       "      <td>64.583928</td>\n",
       "      <td>86.233774</td>\n",
       "      <td>63.914891</td>\n",
       "      <td>...</td>\n",
       "      <td>264.233796</td>\n",
       "      <td>332.963327</td>\n",
       "      <td>459.148015</td>\n",
       "      <td>604.253136</td>\n",
       "      <td>530.069768</td>\n",
       "      <td>254.265556</td>\n",
       "      <td>261.680376</td>\n",
       "      <td>309.459442</td>\n",
       "      <td>322.124846</td>\n",
       "      <td>459.235073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            C01_11       C01_12       C02_11       C02_12       C03_11  \\\n",
       "count  3841.000000  3841.000000  3841.000000  3841.000000  3841.000000   \n",
       "mean     29.608315    24.635253    29.937969    37.304354    22.586074   \n",
       "std      10.319607    13.461991    14.349422    27.618238     8.450348   \n",
       "min      10.465534     3.768582     7.828024     4.146649     6.015019   \n",
       "25%      22.565670    15.123158    19.309250    16.097841    15.683448   \n",
       "50%      27.413607    22.129791    26.309766    25.797659    22.756174   \n",
       "75%      37.076994    31.842144    37.404580    53.354334    28.569266   \n",
       "max      63.479527    74.167295    88.395436   115.503629    61.589324   \n",
       "\n",
       "            C03_12       C04_11       C04_12       C05_11       C05_12  ...  \\\n",
       "count  3841.000000  3841.000000  3841.000000  3841.000000  3841.000000  ...   \n",
       "mean     30.466801    33.041917    30.190067    38.624101    27.533153  ...   \n",
       "std      12.839783    14.240844    13.321235    16.995977    11.984692  ...   \n",
       "min      11.547645     7.665631    11.954557     8.443727     6.249616  ...   \n",
       "25%      20.843118    23.109047    18.031392    24.798825    18.709808  ...   \n",
       "50%      29.334299    32.367934    27.506147    37.733462    26.168477  ...   \n",
       "75%      35.338651    41.385368    37.609440    49.933597    36.194390  ...   \n",
       "max      71.039395    72.117287    64.583928    86.233774    63.914891  ...   \n",
       "\n",
       "       control2_01  control2_02  control3_01  control3_02  control4_01  \\\n",
       "count  3841.000000  3841.000000  3841.000000  3841.000000  3841.000000   \n",
       "mean     59.642004    31.082821    64.339435    41.205316    99.378109   \n",
       "std      48.980261    17.824924    74.473012    42.341037   104.077622   \n",
       "min      12.247924     7.916369    11.581669    11.966519    20.971488   \n",
       "25%      32.875997    18.972532    34.008533    23.128025    52.022034   \n",
       "50%      47.727315    29.322149    49.708925    31.735030    73.901908   \n",
       "75%      65.736838    37.287423    63.184969    46.008286    94.935978   \n",
       "max     264.233796   332.963327   459.148015   604.253136   530.069768   \n",
       "\n",
       "       control4_02  control6_01  control6_02  control7_01  control7_02  \n",
       "count  3841.000000  3841.000000  3841.000000  3841.000000  3841.000000  \n",
       "mean     41.428295    53.405596    38.037733    40.292484    36.764331  \n",
       "std      19.682270    54.791133    13.718908    28.194947    28.651363  \n",
       "min       9.040138    12.812669    11.670371    11.576587     9.365653  \n",
       "25%      24.981298    23.224120    29.212238    23.301624    22.219017  \n",
       "50%      39.825411    38.909549    38.428227    33.536709    29.230624  \n",
       "75%      55.150159    54.394438    47.324724    46.853046    40.857373  \n",
       "max     254.265556   261.680376   309.459442   322.124846   459.235073  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolled =residues.rolling(round(4096/16)).std().dropna()\n",
    "rolled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_serie_windowing(ts, label, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tf.constant(ts))\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(window_size))\n",
    "    ds = ds.map(lambda window: (window, [label]))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window function is applied to Dataset objects. Same windows lenghts for each time series corresponding to each subject are built asigning the rigth label. Then they are included in same Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(1,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#windowing parameters\n",
    "x = rolled.T.values.astype('float32')\n",
    "y=labels.T.values.astype('int8')\n",
    "shuffle_buffer = num_features\n",
    "\n",
    "#series, window_size, batch_size,label\n",
    "\n",
    "for i,subject in enumerate(zip(x,y)):\n",
    "    new = time_serie_windowing(subject[0], subject[1], window_size)\n",
    "    if i>0:\n",
    "        dataset = tf.data.Dataset.concatenate(dataset,new)\n",
    "    else:\n",
    "        print(\"new\")\n",
    "        dataset = new\n",
    "dataset.cache()\n",
    "    #print(fetaures, label)\n",
    "\n",
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,) tf.Tensor([1], shape=(1,), dtype=int8)\n",
      "(1024,) tf.Tensor([1], shape=(1,), dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "#list(dataset.as_numpy_iterator())[0]\n",
    "for features, label in new.take(2):\n",
    "    print(features.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2817\n"
     ]
    }
   ],
   "source": [
    "for idx, reg in enumerate(new):\n",
    "    pass\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.experimental.cardinality(dataset).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140899"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE=idx\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "batch_size = 8\n",
    "\n",
    "full_dataset = dataset\n",
    "full_dataset = full_dataset.shuffle(DATASET_SIZE)\n",
    "train_dataset = full_dataset.take(train_size).batch(batch_size).cache()\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size).batch(batch_size).cache()\n",
    "test_dataset = test_dataset.take(test_size).batch(batch_size).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 1), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b86cad8e9d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ts, label in train_dataset.take(1):\n",
    "    print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN SIMPLE ARCHITECTURE ANALISYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Early stop configuration\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_accuracy', min_delta=1e-3,\n",
    "  patience=200)\n",
    "\n",
    "training_earlystop_callback = EarlyStopping(\n",
    "  monitor='accuracy', min_delta=1e-4,\n",
    "  patience=200)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        earlystop_callback,\n",
    "        #tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, min_delta=1e-5),\n",
    "        #tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, train_dataset, test_dataset, name, optimizer=None, max_epochs=1000):\n",
    "    tf.keras.backend.clear_session()# avoid clutter from old models and layers, especially when memory is limited\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "    history = model.fit(train_dataset, \n",
    "                        use_multiprocessing=True, \n",
    "                        validation_data=test_dataset, \n",
    "                        epochs=max_epochs, \n",
    "                        callbacks=get_callbacks(name),\n",
    "                        verbose=1, \n",
    "                        shuffle=True)\n",
    "    return history\n",
    "\n",
    "# Many models train better if you gradually reduce the learning rate during training. \n",
    "# Use optimizers.schedules to reduce the learning rate over time:\n",
    "def get_optimizer(steps_per_epoch=1, lr=1e-4, multiplier=1000):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(lr,\n",
    "                                                                 decay_steps=steps_per_epoch*multiplier,\n",
    "                                                                 decay_rate=1,\n",
    "                                                                 staircase=False)\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models train better if you gradually reduce the learning rate during training. \n",
    "Use optimizers.schedules to reduce the learning rate over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_histories = {}\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu, input_shape=(window_size,)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                19704     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 19,729\n",
      "Trainable params: 19,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10000\n",
      "13222/13222 [==============================] - 132s 6ms/step - loss: 1.5626 - accuracy: 0.5328 - val_loss: 0.8834 - val_accuracy: 0.5184\n",
      "\n",
      "Epoch: 0, accuracy:0.5328,  loss:1.5626,  val_accuracy:0.5184,  val_loss:0.8834,  \n",
      ".Epoch 2/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.8546 - accuracy: 0.5239 - val_loss: 0.8003 - val_accuracy: 0.5085\n",
      ".Epoch 3/10000\n",
      "13222/13222 [==============================] - 16s 1ms/step - loss: 0.7873 - accuracy: 0.5185 - val_loss: 0.7706 - val_accuracy: 0.5043\n",
      ".Epoch 4/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7620 - accuracy: 0.5144 - val_loss: 0.7531 - val_accuracy: 0.5031\n",
      ".Epoch 5/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7486 - accuracy: 0.5129 - val_loss: 0.7442 - val_accuracy: 0.5017\n",
      ".Epoch 6/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7403 - accuracy: 0.5107 - val_loss: 0.7371 - val_accuracy: 0.5000\n",
      ".Epoch 7/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7349 - accuracy: 0.5091 - val_loss: 0.7316 - val_accuracy: 0.5017\n",
      ".Epoch 8/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7308 - accuracy: 0.5086 - val_loss: 0.7274 - val_accuracy: 0.5011\n",
      ".Epoch 9/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7276 - accuracy: 0.5080 - val_loss: 0.7239 - val_accuracy: 0.5014\n",
      ".Epoch 10/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7250 - accuracy: 0.5077 - val_loss: 0.7210 - val_accuracy: 0.5010\n",
      ".Epoch 11/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7229 - accuracy: 0.5071 - val_loss: 0.7186 - val_accuracy: 0.5016\n",
      ".Epoch 12/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7211 - accuracy: 0.5060 - val_loss: 0.7166 - val_accuracy: 0.5015\n",
      ".Epoch 13/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7195 - accuracy: 0.5058 - val_loss: 0.7149 - val_accuracy: 0.5022\n",
      ".Epoch 14/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7182 - accuracy: 0.5061 - val_loss: 0.7134 - val_accuracy: 0.5019\n",
      ".Epoch 15/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7170 - accuracy: 0.5056 - val_loss: 0.7122 - val_accuracy: 0.5018\n",
      ".Epoch 16/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7159 - accuracy: 0.5054 - val_loss: 0.7111 - val_accuracy: 0.5028\n",
      ".Epoch 17/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7150 - accuracy: 0.5049 - val_loss: 0.7102 - val_accuracy: 0.5017\n",
      ".Epoch 18/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7141 - accuracy: 0.5046 - val_loss: 0.7094 - val_accuracy: 0.5018\n",
      ".Epoch 19/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7134 - accuracy: 0.5045 - val_loss: 0.7086 - val_accuracy: 0.5013\n",
      ".Epoch 20/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7126 - accuracy: 0.5044 - val_loss: 0.7080 - val_accuracy: 0.5015\n",
      ".Epoch 21/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7120 - accuracy: 0.5042 - val_loss: 0.7074 - val_accuracy: 0.5017\n",
      ".Epoch 22/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7114 - accuracy: 0.5042 - val_loss: 0.7069 - val_accuracy: 0.5016\n",
      ".Epoch 23/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7108 - accuracy: 0.5038 - val_loss: 0.7064 - val_accuracy: 0.5020\n",
      ".Epoch 24/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7103 - accuracy: 0.5036 - val_loss: 0.7060 - val_accuracy: 0.5019\n",
      ".Epoch 25/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7098 - accuracy: 0.5036 - val_loss: 0.7056 - val_accuracy: 0.5023\n",
      ".Epoch 26/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7093 - accuracy: 0.5034 - val_loss: 0.7052 - val_accuracy: 0.5018\n",
      ".Epoch 27/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7089 - accuracy: 0.5034 - val_loss: 0.7048 - val_accuracy: 0.5026\n",
      ".Epoch 28/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7084 - accuracy: 0.5034 - val_loss: 0.7044 - val_accuracy: 0.5024\n",
      ".Epoch 29/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7080 - accuracy: 0.5031 - val_loss: 0.7041 - val_accuracy: 0.5022\n",
      ".Epoch 30/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7076 - accuracy: 0.5024 - val_loss: 0.7038 - val_accuracy: 0.5031\n",
      ".Epoch 31/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7072 - accuracy: 0.5026 - val_loss: 0.7035 - val_accuracy: 0.5029\n",
      ".Epoch 32/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7069 - accuracy: 0.5022 - val_loss: 0.7032 - val_accuracy: 0.5026\n",
      ".Epoch 33/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7065 - accuracy: 0.5025 - val_loss: 0.7030 - val_accuracy: 0.5025\n",
      ".Epoch 34/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7062 - accuracy: 0.5023 - val_loss: 0.7027 - val_accuracy: 0.5025\n",
      ".Epoch 35/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7059 - accuracy: 0.5024 - val_loss: 0.7025 - val_accuracy: 0.5026\n",
      ".Epoch 36/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7056 - accuracy: 0.5021 - val_loss: 0.7022 - val_accuracy: 0.5027\n",
      ".Epoch 37/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7054 - accuracy: 0.5020 - val_loss: 0.7020 - val_accuracy: 0.5030\n",
      ".Epoch 38/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7051 - accuracy: 0.5020 - val_loss: 0.7018 - val_accuracy: 0.5028\n",
      ".Epoch 39/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7048 - accuracy: 0.5015 - val_loss: 0.7016 - val_accuracy: 0.5032\n",
      ".Epoch 40/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7046 - accuracy: 0.5016 - val_loss: 0.7014 - val_accuracy: 0.5032\n",
      ".Epoch 41/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7044 - accuracy: 0.5017 - val_loss: 0.7012 - val_accuracy: 0.5034\n",
      ".Epoch 42/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7041 - accuracy: 0.5017 - val_loss: 0.7010 - val_accuracy: 0.5034\n",
      ".Epoch 43/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7039 - accuracy: 0.5018 - val_loss: 0.7008 - val_accuracy: 0.5037\n",
      ".Epoch 44/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7037 - accuracy: 0.5018 - val_loss: 0.7007 - val_accuracy: 0.5037\n",
      ".Epoch 45/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7035 - accuracy: 0.5016 - val_loss: 0.7005 - val_accuracy: 0.5038\n",
      ".Epoch 46/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7033 - accuracy: 0.5017 - val_loss: 0.7003 - val_accuracy: 0.5038\n",
      ".Epoch 47/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7031 - accuracy: 0.5017 - val_loss: 0.7001 - val_accuracy: 0.5037\n",
      ".Epoch 48/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7029 - accuracy: 0.5019 - val_loss: 0.7000 - val_accuracy: 0.5035\n",
      ".Epoch 49/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7027 - accuracy: 0.5019 - val_loss: 0.6998 - val_accuracy: 0.5032\n",
      ".Epoch 50/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7026 - accuracy: 0.5018 - val_loss: 0.6997 - val_accuracy: 0.5033\n",
      ".Epoch 51/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7024 - accuracy: 0.5017 - val_loss: 0.6995 - val_accuracy: 0.5037\n",
      ".Epoch 52/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7022 - accuracy: 0.5017 - val_loss: 0.6994 - val_accuracy: 0.5036\n",
      ".Epoch 53/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7021 - accuracy: 0.5016 - val_loss: 0.6992 - val_accuracy: 0.5036\n",
      ".Epoch 54/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7019 - accuracy: 0.5018 - val_loss: 0.6991 - val_accuracy: 0.5034\n",
      ".Epoch 55/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7018 - accuracy: 0.5018 - val_loss: 0.6989 - val_accuracy: 0.5032\n",
      ".Epoch 56/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7016 - accuracy: 0.5018 - val_loss: 0.6988 - val_accuracy: 0.5029\n",
      ".Epoch 57/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7015 - accuracy: 0.5018 - val_loss: 0.6987 - val_accuracy: 0.5028\n",
      ".Epoch 58/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7013 - accuracy: 0.5018 - val_loss: 0.6986 - val_accuracy: 0.5028\n",
      ".Epoch 59/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7012 - accuracy: 0.5018 - val_loss: 0.6984 - val_accuracy: 0.5027\n",
      ".Epoch 60/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7010 - accuracy: 0.5018 - val_loss: 0.6983 - val_accuracy: 0.5028\n",
      ".Epoch 61/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7009 - accuracy: 0.5018 - val_loss: 0.6982 - val_accuracy: 0.5027\n",
      ".Epoch 62/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7008 - accuracy: 0.5019 - val_loss: 0.6981 - val_accuracy: 0.5026\n",
      ".Epoch 63/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7007 - accuracy: 0.5018 - val_loss: 0.6980 - val_accuracy: 0.5023\n",
      ".Epoch 64/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7005 - accuracy: 0.5018 - val_loss: 0.6979 - val_accuracy: 0.5022\n",
      ".Epoch 65/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7004 - accuracy: 0.5018 - val_loss: 0.6978 - val_accuracy: 0.5023\n",
      ".Epoch 66/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7003 - accuracy: 0.5017 - val_loss: 0.6977 - val_accuracy: 0.5022\n",
      ".Epoch 67/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.7002 - accuracy: 0.5018 - val_loss: 0.6976 - val_accuracy: 0.5022\n",
      ".Epoch 68/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.7001 - accuracy: 0.5018 - val_loss: 0.6975 - val_accuracy: 0.5021\n",
      ".Epoch 69/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6999 - accuracy: 0.5018 - val_loss: 0.6974 - val_accuracy: 0.5020\n",
      ".Epoch 70/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6998 - accuracy: 0.5018 - val_loss: 0.6973 - val_accuracy: 0.5021\n",
      ".Epoch 71/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6997 - accuracy: 0.5018 - val_loss: 0.6972 - val_accuracy: 0.5020\n",
      ".Epoch 72/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6996 - accuracy: 0.5018 - val_loss: 0.6971 - val_accuracy: 0.5022\n",
      ".Epoch 73/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6995 - accuracy: 0.5018 - val_loss: 0.6970 - val_accuracy: 0.5024\n",
      ".Epoch 74/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6994 - accuracy: 0.5017 - val_loss: 0.6969 - val_accuracy: 0.5022\n",
      ".Epoch 75/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6993 - accuracy: 0.5016 - val_loss: 0.6968 - val_accuracy: 0.5022\n",
      ".Epoch 76/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6992 - accuracy: 0.5016 - val_loss: 0.6967 - val_accuracy: 0.5023\n",
      ".Epoch 77/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6991 - accuracy: 0.5016 - val_loss: 0.6967 - val_accuracy: 0.5024\n",
      ".Epoch 78/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6990 - accuracy: 0.5016 - val_loss: 0.6966 - val_accuracy: 0.5022\n",
      ".Epoch 79/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6989 - accuracy: 0.5016 - val_loss: 0.6965 - val_accuracy: 0.5021\n",
      ".Epoch 80/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6988 - accuracy: 0.5015 - val_loss: 0.6964 - val_accuracy: 0.5020\n",
      ".Epoch 81/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6988 - accuracy: 0.5015 - val_loss: 0.6963 - val_accuracy: 0.5022\n",
      ".Epoch 82/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6987 - accuracy: 0.5014 - val_loss: 0.6963 - val_accuracy: 0.5022\n",
      ".Epoch 83/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6986 - accuracy: 0.5013 - val_loss: 0.6962 - val_accuracy: 0.5023\n",
      ".Epoch 84/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6985 - accuracy: 0.5013 - val_loss: 0.6961 - val_accuracy: 0.5025\n",
      ".Epoch 85/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6984 - accuracy: 0.5014 - val_loss: 0.6960 - val_accuracy: 0.5023\n",
      ".Epoch 86/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6983 - accuracy: 0.5014 - val_loss: 0.6960 - val_accuracy: 0.5023\n",
      ".Epoch 87/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6982 - accuracy: 0.5013 - val_loss: 0.6959 - val_accuracy: 0.5021\n",
      ".Epoch 88/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6982 - accuracy: 0.5013 - val_loss: 0.6958 - val_accuracy: 0.5024\n",
      ".Epoch 89/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6981 - accuracy: 0.5014 - val_loss: 0.6958 - val_accuracy: 0.5023\n",
      ".Epoch 90/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6980 - accuracy: 0.5015 - val_loss: 0.6957 - val_accuracy: 0.5022\n",
      ".Epoch 91/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6979 - accuracy: 0.5015 - val_loss: 0.6956 - val_accuracy: 0.5023\n",
      ".Epoch 92/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6979 - accuracy: 0.5015 - val_loss: 0.6956 - val_accuracy: 0.5022\n",
      ".Epoch 93/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6978 - accuracy: 0.5015 - val_loss: 0.6955 - val_accuracy: 0.5020\n",
      ".Epoch 94/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6977 - accuracy: 0.5015 - val_loss: 0.6954 - val_accuracy: 0.5021\n",
      ".Epoch 95/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6977 - accuracy: 0.5015 - val_loss: 0.6954 - val_accuracy: 0.5021\n",
      ".Epoch 96/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6976 - accuracy: 0.5014 - val_loss: 0.6953 - val_accuracy: 0.5021\n",
      ".Epoch 97/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6975 - accuracy: 0.5014 - val_loss: 0.6953 - val_accuracy: 0.5020\n",
      ".Epoch 98/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6974 - accuracy: 0.5013 - val_loss: 0.6952 - val_accuracy: 0.5019\n",
      ".Epoch 99/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6974 - accuracy: 0.5014 - val_loss: 0.6951 - val_accuracy: 0.5020\n",
      ".Epoch 100/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6973 - accuracy: 0.5014 - val_loss: 0.6951 - val_accuracy: 0.5019\n",
      ".Epoch 101/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6972 - accuracy: 0.5014 - val_loss: 0.6950 - val_accuracy: 0.5017\n",
      "\n",
      "Epoch: 100, accuracy:0.5014,  loss:0.6972,  val_accuracy:0.5017,  val_loss:0.6950,  \n",
      ".Epoch 102/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6972 - accuracy: 0.5015 - val_loss: 0.6950 - val_accuracy: 0.5017\n",
      ".Epoch 103/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6971 - accuracy: 0.5015 - val_loss: 0.6949 - val_accuracy: 0.5017\n",
      ".Epoch 104/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6971 - accuracy: 0.5015 - val_loss: 0.6949 - val_accuracy: 0.5017\n",
      ".Epoch 105/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6970 - accuracy: 0.5015 - val_loss: 0.6948 - val_accuracy: 0.5018\n",
      ".Epoch 106/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6969 - accuracy: 0.5015 - val_loss: 0.6948 - val_accuracy: 0.5018\n",
      ".Epoch 107/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6969 - accuracy: 0.5016 - val_loss: 0.6947 - val_accuracy: 0.5019\n",
      ".Epoch 108/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6968 - accuracy: 0.5017 - val_loss: 0.6947 - val_accuracy: 0.5018\n",
      ".Epoch 109/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6968 - accuracy: 0.5016 - val_loss: 0.6946 - val_accuracy: 0.5017\n",
      ".Epoch 110/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6967 - accuracy: 0.5016 - val_loss: 0.6946 - val_accuracy: 0.5015\n",
      ".Epoch 111/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6966 - accuracy: 0.5016 - val_loss: 0.6945 - val_accuracy: 0.5017\n",
      ".Epoch 112/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6966 - accuracy: 0.5015 - val_loss: 0.6945 - val_accuracy: 0.5017\n",
      ".Epoch 113/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6965 - accuracy: 0.5015 - val_loss: 0.6944 - val_accuracy: 0.5019\n",
      ".Epoch 114/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6965 - accuracy: 0.5015 - val_loss: 0.6944 - val_accuracy: 0.5017\n",
      ".Epoch 115/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6964 - accuracy: 0.5014 - val_loss: 0.6943 - val_accuracy: 0.5015\n",
      ".Epoch 116/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6964 - accuracy: 0.5014 - val_loss: 0.6943 - val_accuracy: 0.5017\n",
      ".Epoch 117/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6963 - accuracy: 0.5014 - val_loss: 0.6942 - val_accuracy: 0.5017\n",
      ".Epoch 118/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6963 - accuracy: 0.5013 - val_loss: 0.6942 - val_accuracy: 0.5016\n",
      ".Epoch 119/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6962 - accuracy: 0.5013 - val_loss: 0.6941 - val_accuracy: 0.5016\n",
      ".Epoch 120/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6962 - accuracy: 0.5014 - val_loss: 0.6941 - val_accuracy: 0.5014\n",
      ".Epoch 121/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6961 - accuracy: 0.5014 - val_loss: 0.6940 - val_accuracy: 0.5017\n",
      ".Epoch 122/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6961 - accuracy: 0.5015 - val_loss: 0.6940 - val_accuracy: 0.5017\n",
      ".Epoch 123/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6960 - accuracy: 0.5015 - val_loss: 0.6939 - val_accuracy: 0.5017\n",
      ".Epoch 124/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6960 - accuracy: 0.5015 - val_loss: 0.6939 - val_accuracy: 0.5017\n",
      ".Epoch 125/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6959 - accuracy: 0.5016 - val_loss: 0.6938 - val_accuracy: 0.5016\n",
      ".Epoch 126/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6959 - accuracy: 0.5015 - val_loss: 0.6938 - val_accuracy: 0.5016\n",
      ".Epoch 127/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6958 - accuracy: 0.5016 - val_loss: 0.6938 - val_accuracy: 0.5016\n",
      ".Epoch 128/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6958 - accuracy: 0.5016 - val_loss: 0.6937 - val_accuracy: 0.5015\n",
      ".Epoch 129/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6957 - accuracy: 0.5018 - val_loss: 0.6937 - val_accuracy: 0.5016\n",
      ".Epoch 130/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6957 - accuracy: 0.5018 - val_loss: 0.6936 - val_accuracy: 0.5017\n",
      ".Epoch 131/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6956 - accuracy: 0.5018 - val_loss: 0.6936 - val_accuracy: 0.5017\n",
      ".Epoch 132/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6956 - accuracy: 0.5017 - val_loss: 0.6936 - val_accuracy: 0.5017\n",
      ".Epoch 133/10000\n",
      "13222/13222 [==============================] - 16s 1ms/step - loss: 0.6955 - accuracy: 0.5017 - val_loss: 0.6935 - val_accuracy: 0.5016\n",
      ".Epoch 134/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6955 - accuracy: 0.5018 - val_loss: 0.6935 - val_accuracy: 0.5017\n",
      ".Epoch 135/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6955 - accuracy: 0.5017 - val_loss: 0.6934 - val_accuracy: 0.5017\n",
      ".Epoch 136/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6954 - accuracy: 0.5017 - val_loss: 0.6934 - val_accuracy: 0.5016\n",
      ".Epoch 137/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6954 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.5016\n",
      ".Epoch 138/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6953 - accuracy: 0.5017 - val_loss: 0.6933 - val_accuracy: 0.5016\n",
      ".Epoch 139/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6953 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.5015\n",
      ".Epoch 140/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6952 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.5014\n",
      ".Epoch 141/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6952 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5015\n",
      ".Epoch 142/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6952 - accuracy: 0.5018 - val_loss: 0.6932 - val_accuracy: 0.5015\n",
      ".Epoch 143/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6951 - accuracy: 0.5019 - val_loss: 0.6931 - val_accuracy: 0.5014\n",
      ".Epoch 144/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6951 - accuracy: 0.5019 - val_loss: 0.6931 - val_accuracy: 0.5015\n",
      ".Epoch 145/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6950 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5014\n",
      ".Epoch 146/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6950 - accuracy: 0.5020 - val_loss: 0.6930 - val_accuracy: 0.5014\n",
      ".Epoch 147/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6950 - accuracy: 0.5020 - val_loss: 0.6930 - val_accuracy: 0.5015\n",
      ".Epoch 148/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6949 - accuracy: 0.5020 - val_loss: 0.6929 - val_accuracy: 0.5016\n",
      ".Epoch 149/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6949 - accuracy: 0.5020 - val_loss: 0.6929 - val_accuracy: 0.5017\n",
      ".Epoch 150/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6948 - accuracy: 0.5020 - val_loss: 0.6929 - val_accuracy: 0.5016\n",
      ".Epoch 151/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6948 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5017\n",
      ".Epoch 152/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6948 - accuracy: 0.5020 - val_loss: 0.6928 - val_accuracy: 0.5016\n",
      ".Epoch 153/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6947 - accuracy: 0.5021 - val_loss: 0.6928 - val_accuracy: 0.5015\n",
      ".Epoch 154/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6947 - accuracy: 0.5021 - val_loss: 0.6927 - val_accuracy: 0.5017\n",
      ".Epoch 155/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6947 - accuracy: 0.5021 - val_loss: 0.6927 - val_accuracy: 0.5017\n",
      ".Epoch 156/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6946 - accuracy: 0.5022 - val_loss: 0.6927 - val_accuracy: 0.5017\n",
      ".Epoch 157/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6946 - accuracy: 0.5022 - val_loss: 0.6926 - val_accuracy: 0.5017\n",
      ".Epoch 158/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6945 - accuracy: 0.5022 - val_loss: 0.6926 - val_accuracy: 0.5017\n",
      ".Epoch 159/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6945 - accuracy: 0.5023 - val_loss: 0.6926 - val_accuracy: 0.5017\n",
      ".Epoch 160/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6945 - accuracy: 0.5023 - val_loss: 0.6925 - val_accuracy: 0.5017\n",
      ".Epoch 161/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6944 - accuracy: 0.5024 - val_loss: 0.6925 - val_accuracy: 0.5017\n",
      ".Epoch 162/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6944 - accuracy: 0.5023 - val_loss: 0.6925 - val_accuracy: 0.5016\n",
      ".Epoch 163/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6944 - accuracy: 0.5024 - val_loss: 0.6925 - val_accuracy: 0.5015\n",
      ".Epoch 164/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6943 - accuracy: 0.5023 - val_loss: 0.6924 - val_accuracy: 0.5017\n",
      ".Epoch 165/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6943 - accuracy: 0.5023 - val_loss: 0.6924 - val_accuracy: 0.5017\n",
      ".Epoch 166/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6943 - accuracy: 0.5024 - val_loss: 0.6924 - val_accuracy: 0.5017\n",
      ".Epoch 167/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6942 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5017\n",
      ".Epoch 168/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6942 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5017\n",
      ".Epoch 169/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6942 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5015\n",
      ".Epoch 170/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6941 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5016\n",
      ".Epoch 171/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6941 - accuracy: 0.5024 - val_loss: 0.6922 - val_accuracy: 0.5015\n",
      ".Epoch 172/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6941 - accuracy: 0.5024 - val_loss: 0.6922 - val_accuracy: 0.5016\n",
      ".Epoch 173/10000\n",
      "13222/13222 [==============================] - 14s 1ms/step - loss: 0.6940 - accuracy: 0.5024 - val_loss: 0.6922 - val_accuracy: 0.5017\n",
      ".Epoch 174/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6940 - accuracy: 0.5023 - val_loss: 0.6921 - val_accuracy: 0.5016\n",
      ".Epoch 175/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6940 - accuracy: 0.5023 - val_loss: 0.6921 - val_accuracy: 0.5016\n",
      ".Epoch 176/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6940 - accuracy: 0.5025 - val_loss: 0.6921 - val_accuracy: 0.5015\n",
      ".Epoch 177/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6939 - accuracy: 0.5025 - val_loss: 0.6920 - val_accuracy: 0.5016\n",
      ".Epoch 178/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6939 - accuracy: 0.5026 - val_loss: 0.6920 - val_accuracy: 0.5018\n",
      ".Epoch 179/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6939 - accuracy: 0.5026 - val_loss: 0.6920 - val_accuracy: 0.5017\n",
      ".Epoch 180/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6938 - accuracy: 0.5026 - val_loss: 0.6920 - val_accuracy: 0.5017\n",
      ".Epoch 181/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6938 - accuracy: 0.5026 - val_loss: 0.6919 - val_accuracy: 0.5017\n",
      ".Epoch 182/10000\n",
      "13222/13222 [==============================] - 15s 1ms/step - loss: 0.6938 - accuracy: 0.5026 - val_loss: 0.6919 - val_accuracy: 0.5019\n",
      ".Epoch 183/10000\n",
      "13206/13222 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.5027"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2ca355a11809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                               \u001b[0;34m\"fcnn/tiny\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                               max_epochs=num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-48e16e455857>\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, train_dataset, test_dataset, name, optimizer, max_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         shuffle=True)\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "size_histories['fcnn/tiny'] = compile_and_fit(tiny, train_dataset, \n",
    "                                              val_dataset,\n",
    "                                              \"fcnn/tiny\", \n",
    "                                              optimizer=get_optimizer(), \n",
    "                                              max_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(size_histories['fcnn/tiny'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_model = tf.keras.Sequential([\n",
    "    # `input_shape` is only required here so that `.summary` works.\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(window_size,)),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "size_histories['fcnn/small'] = compile_and_fit(small_model, train_dataset, \n",
    "                                              test_dataset,'fcnn/small',optimizer=get_optimizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(size_histories['fcnn/small'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(window_size,)),  \n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "size_histories['fcnn/large'] = compile_and_fit(large_model, train_dataset, \n",
    "                                              test_dataset, \"fcnn/large\",optimizer=get_optimizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(size_histories['fcnn/large'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(size_histories)\n",
    "plt.ylim([0., 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(size_histories)\n",
    "plt.ylim([0., 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([0.1, max(plt.xlim())])\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([5, max(plt.xlim())])\n",
    "plt.ylim([0.0, 1.])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(FEATURES,)),  \n",
    "    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "size_histories['fcnn/tiny/dropout'] = compile_and_fit(tiny_dropout,  train_dataset, \n",
    "                                                      test_dataset,\"fcnn/tiny/dropout\", \n",
    "                                                      optimizer=None, \n",
    "                                                      max_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(size_histories['fcnn/tiny/dropout'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(size_histories)\n",
    "plt.ylim([0., 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(size_histories)\n",
    "plt.ylim([0., 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the initial model was not overfitting dropout effect is far away from being something positive for training this network. Let's analyse kernel regularization effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(logdir/'regularizers/tiny', ignore_errors=True)\n",
    "# shutil.copytree(logdir/'fcnn/tiny', logdir/'regularizers/tiny')\n",
    "regularizer_histories = {}\n",
    "regularizer_histories['large'] = size_histories['fcnn/large']\n",
    "regularizer_histories['dropout'] = size_histories['fcnn/tiny/dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    "L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "\n",
    "L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    "\n",
    "L1 regularization pushes weights towards exactly zero encouraging a sparse model. L2 regularization will penalize the weights parameters without making them sparse since the penalty goes to zero for small weights. one reason why L2 is more common.\n",
    "\n",
    "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dense(64, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "regularizer_histories['l2'] = compile_and_fit(l2_model, train_dataset, \n",
    "                                                      test_dataset, \"regularizers/l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(regularizer_histories['l2'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(regularizer_histories)\n",
    "plt.ylim([0., 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(regularizer_histories)\n",
    "plt.ylim([0.1, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "combined_model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2),\n",
    "                 input_shape=(FEATURES,)),\n",
    "    layers.Dense(64, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dropout(dr),\n",
    "    layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "regularizer_histories['combined'] = compile_and_fit(combined_model, train_dataset, \n",
    "                                                      test_dataset, \"regularizers/combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(regularizer_histories['combined'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(regularizer_histories)\n",
    "plt.ylim([0., 0.9])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(regularizer_histories)\n",
    "plt.ylim([0.4, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_histories = {}\n",
    "lstm_histories['large'] = size_histories['fcnn/large']\n",
    "lstm_histories['dropout'] = size_histories['fcnn/tiny/dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr=0.2\n",
    "lr2=1e-8\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1),# expand the dimension form (50, 4096) to (50, 4096, 1)\n",
    "                      input_shape=[FEATURES,]),\n",
    "    tf.keras.layers.LSTM(512, activation=tf.nn.tanh, return_sequences=True,\n",
    "                  kernel_regularizer=regularizers.l2(lr2)),\n",
    "#     layers.Dropout(dr),\n",
    "    tf.keras.layers.LSTM(512, activation=tf.nn.tanh,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "#     layers.Dropout(dr),\n",
    "#     tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "#                  kernel_regularizer=regularizers.l2(lr2)),  \n",
    "#     layers.Dropout(dr),\n",
    "#     tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "#                  kernel_regularizer=regularizers.l2(lr2)),\n",
    "#     layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "#     layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(24, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l2(lr2)),\n",
    "    layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_histories['lstm/large'] = compile_and_fit(lstm_large_model, train_dataset, \n",
    "                                                      test_dataset, \"lstm/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(lstm_histories['lstm/large'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(lstm_histories, \"loss\")\n",
    "plt.ylim([0., 0.9])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(lstm_histories)\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
