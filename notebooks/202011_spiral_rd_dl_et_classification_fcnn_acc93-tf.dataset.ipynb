{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-491e58acced5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m35.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'config'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_docs as tfdocs\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from utils import config\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [35.0, 7.0]\n",
    "HDFStore=pd.HDFStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data parameters\n",
    "num_coefficients=int(config[\"coefficients\"])\n",
    "root_path=config[\"root_path\"]\n",
    "hw_results_path= config[\"hw_results_path\"]\n",
    "\n",
    "# training parameters\n",
    "seed=int(config[\"seed\"]) if \"seed\" in config.keys() else 42\n",
    "\n",
    "dr=float(config[\"dropout\"]) if \"dropout\" in config.keys() else 0.2\n",
    "lr2=float(config[\"lr2\"]) if \"lr2\" in config.keys() else 1e-3\n",
    "lr1=float(config[\"lr1\"]) if \"lr1\" in config.keys() else 1e-4\n",
    "lr=float(config[\"lr\"]) if \"lr\" in config.keys() else 8e-4\n",
    "\n",
    "num_epochs=int(config[\"num_epochs\"]) if \"num_epochs\" in config.keys() else 1000\n",
    "num_features=int(config[\"features\"]) if \"features\" in config.keys() else 4096\n",
    "mini_batch_size=int(config[\"mini_batch_size\"]) if \"mini_batch_size\" in config.keys() else 4\n",
    "\n",
    "main_units=int(config[\"main_units\"]) if \"main_units\" in config.keys() else 64\n",
    "secondary_units=int(config[\"secondary_units\"]) if \"secondary_units\" in config.keys() else 16\n",
    "last_unit=int(config[\"last_unit\"]) if \"last_unit\" in config.keys() else 8\n",
    "lstm_units=int(config[\"lstm_units\"]) if \"lstm_units\" in config.keys() else 64\n",
    "num_classes=int(config[\"num_classes\"]) if \"num_classes\" in config.keys() else 1\n",
    "\n",
    "\n",
    "print_sample=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from hdf5 file\n",
    "rdo_root_path = path.join(root_path,hw_results_path)\n",
    "h5file = path.join(rdo_root_path, \"archimedean-\")\n",
    "h5filename = h5file + str(num_coefficients) + \"-splits.h5\"\n",
    "print(h5filename)\n",
    "hdf = HDFStore(h5filename)\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and scale timeseries between 0 and 1\n",
    "x_train = hdf[\"/residues/train/features\"].values.astype('float32')\n",
    "y_train = hdf[\"/residues/train/labels\"].values.astype('int8').reshape(-1,1)\n",
    "x_test = hdf[\"/residues/test/features\"].values.astype('float32')\n",
    "y_test = hdf[\"/residues/test/labels\"].values.astype('int8').reshape(-1,1)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).take(len(x_train)).batch(mini_batch_size).prefetch(2).cache()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).take(len(x_test)).batch(mini_batch_size).prefetch(2).cache()\n",
    "steps_per_epoch = round(len(train_dataset)/mini_batch_size)\n",
    "\n",
    "if print_sample:\n",
    "    for feat, targ in test_dataset.take(10):\n",
    "        print ('Features test: {}, Target: {}'.format(feat, targ))\n",
    "\n",
    "    for feat, targ in test_dataset.take(10):\n",
    "        print ('Features train: {}, Target: {}'.format(feat, targ))\n",
    "\n",
    "print(\"{0} train batches and {1} test batches of {2} mini batch size and {3} steps per epoch\".format(len(train_dataset), \n",
    "                                                                              len(test_dataset),\n",
    "                                                                              mini_batch_size,\n",
    "                                                                                steps_per_epoch))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN ARCHITECTURE ANALISYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some training helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Early stop configuration\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_accuracy', min_delta=1e-3,\n",
    "  patience=200)\n",
    "\n",
    "training_earlystop_callback = EarlyStopping(\n",
    "  monitor='accuracy', min_delta=1e-4,\n",
    "  patience=200)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        earlystop_callback,\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, min_delta=1e-5),\n",
    "        #tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, train_dataset, test_dataset, name, optimizer=None, max_epochs=1000):\n",
    "    tf.keras.backend.clear_session()# avoid clutter from old models and layers, especially when memory is limited\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "    history = model.fit(train_dataset, \n",
    "                        use_multiprocessing=True, \n",
    "                        validation_data=test_dataset, epochs=max_epochs, \n",
    "                        callbacks=get_callbacks(name),\n",
    "                        verbose=0, shuffle=True)\n",
    "    return history\n",
    "\n",
    "# Many models train better if you gradually reduce the learning rate during training. \n",
    "# Use optimizers.schedules to reduce the learning rate over time:\n",
    "def get_optimizer(steps_per_epoch=1, lr=1e-4, multiplier=1000):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(lr,\n",
    "                                                                 decay_steps=steps_per_epoch*multiplier,\n",
    "                                                                 decay_rate=1,\n",
    "                                                                 staircase=False)\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in size_histories:\n",
    "#     plt.plot(size_histories[key].history[\"accuracy\"])\n",
    "#     plt.legend[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)])\n",
    "\n",
    "size_histories['fcnn/tiny'] = compile_and_fit(tiny, train_dataset, \n",
    "                                              test_dataset,\n",
    "                                              \"fcnn/tiny\", \n",
    "#                                               optimizer=get_optimizer(),\n",
    "                                              optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                               optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "                                              max_epochs=num_epochs)\n",
    "\n",
    "print(\"\\n#######################Evaluation###########################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(size_histories['fcnn/tiny'].history[\"accuracy\"]))\n",
    "print('test acc:', max(size_histories['fcnn/tiny'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = tf.keras.Sequential([\n",
    "    # `input_shape` is only required here so that `.summary` works.\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(secondary_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "size_histories['fcnn/small'] = compile_and_fit(small_model, train_dataset, \n",
    "                                              test_dataset,'fcnn/small',\n",
    "#                                               optimizer=get_optimizer(),\n",
    "                                              optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                               optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "                                              max_epochs=num_epochs)\n",
    "\n",
    "print(\"\\n#######################Evaluation###########################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(size_histories['fcnn/small'].history[\"accuracy\"]))\n",
    "print('test acc:', max(size_histories['fcnn/small'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)),  \n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(secondary_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "size_histories['fcnn/large'] = compile_and_fit(large_model, train_dataset, \n",
    "                                              test_dataset, \"fcnn/large\",\n",
    "                                              optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                               optimizer=get_optimizer(), \n",
    "                                              max_epochs=num_epochs)\n",
    "\n",
    "print(\"Evaluation\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(size_histories['fcnn/large'].history[\"accuracy\"]))\n",
    "print('test acc:', max(size_histories['fcnn/large'].history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(size_histories)\n",
    "plt.ylim([0., 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(size_histories)\n",
    "plt.ylim([0., 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([0.1, max(plt.xlim())])\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([5, max(plt.xlim())])\n",
    "plt.ylim([0.0, 1.])\n",
    "plt.xlabel(\"Epochs [Log Scale]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Although we got an accuracy of 0.75 looking at loss function chart is pretty obvious: There is overfiting. Now we have an overfitted NN let's make some adjustments to improve generalization capacity startin from dropout techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)), \n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(secondary_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "size_histories['fcnn/tiny/dropout'] = compile_and_fit(tiny_dropout, \n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      \"fcnn/tiny/dropout\", \n",
    "                                                      optimizer=tf.keras.optimizers.Adam(lr), \n",
    "                                                      max_epochs=num_epochs)\n",
    "\n",
    "print(\"\\n#######################EVALUATIO######################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(size_histories['fcnn/tiny/dropout'].history[\"accuracy\"]))\n",
    "print('test acc:', max(size_histories['fcnn/tiny/dropout'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss = tfdocs.plots.HistoryPlotter(metric = 'loss', smoothing_std=10)\n",
    "plotter_loss.plot(size_histories)\n",
    "plt.ylim([0., 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc = tfdocs.plots.HistoryPlotter(metric = 'accuracy', smoothing_std=10)\n",
    "plotter_acc.plot(size_histories)\n",
    "plt.ylim([0., 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy keeps improving more epoch with a total improvement from 0.75 to 0.81. Furthemore, the gap between training loss function and test loss function has been reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(logdir/'regularizers/tiny', ignore_errors=True)\n",
    "# shutil.copytree(logdir/'fcnn/tiny', logdir/'regularizers/tiny')\n",
    "regularizer_histories = {}\n",
    "regularizer_histories['large'] = size_histories['fcnn/large']\n",
    "regularizer_histories['dropout'] = size_histories['fcnn/tiny/dropout']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
    "\n",
    "L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
    "\n",
    "L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the squared \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
    "\n",
    "L1 regularization pushes weights towards exactly zero encouraging a sparse model. L2 regularization will penalize the weights parameters without making them sparse since the penalty goes to zero for small weights. one reason why L2 is more common.\n",
    "\n",
    "In tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = tf.keras.Sequential([\n",
    "    layers.Dense(main_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1),\n",
    "                 input_shape=(num_features,)),\n",
    "    layers.Dense(main_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    layers.Dense(secondary_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    layers.Dense(last_unit, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "regularizer_histories['l2'] = compile_and_fit(l2_model, train_dataset, \n",
    "                                              test_dataset, \n",
    "                                              optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                                              name=\"regularizers/l2\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"\\n#######################EVALUATIO######################\")\n",
    "print('test acc:', max(regularizer_histories['l2'].history[\"val_accuracy\"]))\n",
    "print('train acc:', max(regularizer_histories['l2'].history[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(regularizer_histories)\n",
    "plt.ylim([0., 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(regularizer_histories)\n",
    "plt.ylim([0.1, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens combining both techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "combined_model = tf.keras.Sequential([\n",
    "    layers.Dense(main_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1),\n",
    "                 input_shape=(num_features,)),\n",
    "    layers.Dense(main_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    layers.Dense(secondary_units, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    layers.Dense(last_unit, activation=tf.nn.relu,\n",
    "                 kernel_regularizer=regularizers.l1(lr1)),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "regularizer_histories['combined'] = compile_and_fit(combined_model, train_dataset, test_dataset, \"regularizers/combined\",\n",
    "                                                   optimizer=get_optimizer())\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print()\n",
    "print('test acc:', max(regularizer_histories['combined'].history[\"val_accuracy\"]))\n",
    "print('train acc:', max(regularizer_histories['combined'].history[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(regularizer_histories)\n",
    "plt.ylim([0., 0.9])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(regularizer_histories)\n",
    "plt.ylim([0.4, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "[Tensorflow implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_histories = {}\n",
    "batch_histories['large'] = size_histories['fcnn/large']\n",
    "batch_histories['dropout'] = size_histories['fcnn/tiny/dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_batch = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(secondary_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "batch_histories['fcnn/small/batch'] = compile_and_fit(small_batch, \n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      \"fcnn/small/batch\", \n",
    "                                                      optimizer=tf.keras.optimizers.Adam(lr), \n",
    "                                                      max_epochs=num_epochs)\n",
    "\n",
    "print(\"\\n#######################EVALUATION######################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(batch_histories['fcnn/small/batch'].history[\"accuracy\"]))\n",
    "print('test acc:', max(batch_histories['fcnn/small/batch'].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(batch_histories)\n",
    "plt.ylim([0., 0.9])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-FCNN\n",
    "\n",
    "Univariate time series classification model, the Long Short Term Memory Fully Convolutional Network (LSTM-FCN) to capture time related information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_histories = {}\n",
    "lstm_histories['large'] = size_histories['fcnn/large']\n",
    "lstm_histories['dropout'] = size_histories['fcnn/tiny/dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units=24\n",
    "lstm_tiny_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1),# expand the dimension form (50, 4096) to (50, 4096, 1)\n",
    "                      input_shape=[num_features,]),\n",
    "    tf.keras.layers.LSTM(lstm_units, activation=tf.nn.tanh, return_sequences=False,\n",
    "                        kernel_regularizer=regularizers.l2(lr2)\n",
    "                        ),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_histories['lstm/tiny'] = compile_and_fit(lstm_tiny_model, \n",
    "                                               train_dataset, \n",
    "                                               test_dataset, \n",
    "                                               optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                                optimizer=get_optimizer(int(round(size/mini_batch_size)),\n",
    "#                                                                        lr=1e-4,\n",
    "#                                                                        multiplier=10),\n",
    "                                               name=\"lstm/tiny\")\n",
    "\n",
    "print(\"\\n#######################EVALUATION######################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('test acc:', max(lstm_histories['lstm/tiny'].history[\"val_accuracy\"]))\n",
    "print('train acc:', max(lstm_histories['lstm/tiny'].history[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_small_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1),# expand the dimension form (50, 4096) to (50, 4096, 1)\n",
    "                      input_shape=[num_features,]),\n",
    "    tf.keras.layers.LSTM(lstm_units, activation=tf.nn.tanh, return_sequences=True,\n",
    "                        kernel_regularizer=regularizers.l2(lr2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(lstm_units, activation=tf.nn.tanh, return_sequences=False,\n",
    "                        kernel_regularizer=regularizers.l2(lr2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_histories['lstm/small'] = compile_and_fit(lstm_small_model, \n",
    "                                               train_dataset, \n",
    "                                               test_dataset, \n",
    "                                               optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                                optimizer=get_optimizer(int(round(size/mini_batch_size)),\n",
    "#                                                                        lr=1e-4,\n",
    "#                                                                        multiplier=10),\n",
    "                                               name=\"lstm/small\")\n",
    "\n",
    "print(\"\\n#######################EVALUATION######################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(lstm_histories['lstm/small'].history[\"accuracy\"]))\n",
    "print('test acc:', max(lstm_histories['lstm/small'].history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_units=64\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_large_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1),# expand the dimension form (50, 4096) to (50, 4096, 1)\n",
    "                      input_shape=[num_features,]),\n",
    "    tf.keras.layers.LSTM(lstm_units, activation=tf.nn.tanh, return_sequences=True,\n",
    "                        kernel_regularizer=regularizers.l2(lr2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(lstm_units, activation=tf.nn.tanh, return_sequences=False,\n",
    "                        kernel_regularizer=regularizers.l2(lr2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu, input_shape=(num_features,)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(main_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(secondary_units, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(last_unit, activation=tf.nn.relu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(dr),\n",
    "    tf.keras.layers.Dense(num_classes, activation=tf.nn.sigmoid)\n",
    "])\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "lstm_histories['lstm/large'] = compile_and_fit(lstm_large_model, \n",
    "                                               train_dataset, \n",
    "                                               test_dataset, \n",
    "                                               optimizer=tf.keras.optimizers.Adam(lr), \n",
    "#                                                optimizer=get_optimizer(int(round(size/mini_batch_size)),\n",
    "#                                                lr=1e-4,\n",
    "#                                                multiplier=10), \n",
    "                                                name=\"lstm/large\")\n",
    "\n",
    "print(\"\\n#######################EVALUATION######################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(lstm_histories['lstm/large'].history[\"accuracy\"]))\n",
    "print('test acc:', max(lstm_histories['lstm/large'].history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_loss.plot(lstm_histories, \"loss\")\n",
    "plt.ylim([0., 1.2])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_acc.plot(lstm_histories)\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlim([5, max(plt.xlim())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
