{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import h5py\n",
    "import logging\n",
    "from os import path\n",
    "from pandas import HDFStore\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from utils import get_callbacks,windowed_dataset,plot_report, basic_lstm\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "seed=2020\n",
    "tf.random.set_seed(seed) # set up tensorflow's seed\n",
    "np.random.seed(seed) # set up numpy's seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = 22\n",
    "# TODO export load data to data loaders or Utils\n",
    "root_path = \"Z:/elekin\"\n",
    "rdo_root_path = path.join(root_path,\"02-RESULTADOS/03-HANDWRITTING\")\n",
    "h5file = path.join(rdo_root_path, \"00-OUTPUT/archimedean-\")\n",
    "h5filename = h5file + str(17) + \".h5\"\n",
    "hdf = HDFStore(h5filename)\n",
    "\n",
    "h5_outputfile = path.join(rdo_root_path, \"00-OUTPUT/archimedean-\")\n",
    "h5_output_filename = h5_outputfile + str(coeff) + \"-splits\" +\".h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load timeseries and labels\n",
    "raw_df = hdf['results/residues/rd'].T\n",
    "raw_labels_df = hdf.get('source/labels')\n",
    "raw_dataset = raw_df.join(raw_labels_df).sample(frac=1, random_state=seed)# how to join and shuffle two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T012_02        1.0\n",
       "C07_12         0.0\n",
       "T013_02        1.0\n",
       "T029_02        1.0\n",
       "control2_02    0.0\n",
       "Name: labels, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['labels'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window function is applied to Dataset objects. Same windows lenghts for each time series corresponding to each subject are built asigning the rigth label. Then they are included in same Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1024\n",
    "shuffle_buffer = 4000\n",
    "batch_size = 1\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices(raw_df.values.astype(\"float32\"))\n",
    "labels = raw_labels_df.values.astype('int8')\n",
    "\n",
    "for i,features in enumerate(raw_df.values.astype(\"float32\")):\n",
    "    new = windowed_dataset(features,window_size,batch_size,shuffle_buffer,labels[i])\n",
    "    if i>0:\n",
    "        dataset = tf.data.Dataset.concatenate(dataset,new)\n",
    "    else:\n",
    "        dataset = new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int8, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    print(\"x =\",x)\n",
    "    print(\"y =\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply filters to a Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x,y in dataset.filter(lambda x,y: tf.equal(y, 1)[0]).take(1):\n",
    "    print(\"x =\",x)\n",
    "    print(\"y =\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to access just to one element using python iterator. In the example bellow it is showed also how to shuffle the dataset and batch together some examples into minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset.shuffle(shuffle_buffer*20).batch(batch_size))\n",
    "first = iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in first:\n",
    "    print(\"first batch: \",x.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to build different dataset splits for training pruposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = 4096\n",
    "DATASET_SIZE = int((FEATURES/window_size)*50)\n",
    "#TODO import from utils API\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_dataset = dataset\n",
    "full_dataset = full_dataset.shuffle(DATASET_SIZE)\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(test_size)\n",
    "test_dataset = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM basic architectures --> #TODO import from utils API\n",
    "tf.keras.backend.clear_session()# para evitar que entrenamientos annteriores afecten\n",
    "basic_lstm = basic_lstm(8, window_size, dropout=0.5)\n",
    "basic_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=7e-6), \n",
    "                          loss='binary_crossentropy',metrics=['accuracy'])\n",
    "basic_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "#EVALUATION_INTERVAL = 20\n",
    "\n",
    "#history = basic_lstm.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL,\n",
    "#                      validation_data=val_dataset, validation_steps=50, callbacks=get_callbacks(), verbose=0)\n",
    "history = basic_lstm.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_callbacks(), verbose=0)\n",
    "plot_report(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
