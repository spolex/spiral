{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "0. x coordinate\n",
    "1. y coordinate\n",
    "2. timestamp\n",
    "3. pen-up\n",
    "4. azimuth angle\n",
    "5. altitude angle\n",
    "6. Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "\n",
    "#plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#processing\n",
    "from scipy.signal import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['x','y','pen_up','pressure']\n",
    "doc_path = \"/data/elekin/doc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadf=pd.read_csv(path.join(doc_path,\"metadata-202106-v1.csv\"),index_col=0)\n",
    "# metadf[metadf[\"temblor\"]=='si']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with subjects time series and tremor levels\n",
    " \n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for path, level in zip(metadf.abs_path, metadf.level):\n",
    "    df = pd.read_csv(path, sep=\"\\s+\",header=None,names=features,skiprows=1,usecols=[0,1,3,6])\n",
    "    X.append(resample(df.values.astype('int16'), 4096))\n",
    "    y.append(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 15\n",
    "# label = y[idx]\n",
    "# plt.scatter(X[idx][:,0], X[idx][:,1], c=X[idx][:,4], cmap='jet')\n",
    "# _=plt.title(\"Subjet tremor severity group: {}\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 4096, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 38\n",
    "n_outputs = 3\n",
    "mini_batch_size=4\n",
    "\n",
    "n_timesteps = np.array(X).shape[1]\n",
    "n_features = np.array(X).shape[2]\n",
    "\n",
    "data_size = np.array(X).shape[0]\n",
    "train_split = 0.67\n",
    "test_split = 0.33\n",
    "train_size = int(train_split * data_size)\n",
    "test_size = int(test_split * data_size)\n",
    "\n",
    "shuffle_buffer = data_size\n",
    "steps_per_epoch = round(data_size/mini_batch_size)\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n",
    "\n",
    "URI = \"http://192.168.1.12:5001\"\n",
    "\n",
    "mlflow.set_tracking_uri(URI)\n",
    "mlflow.set_experiment('/archimedes-dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, tf.one_hot(y,n_outputs)))\n",
    "full_dataset = dataset.shuffle(shuffle_buffer, seed=seed)\n",
    "train_dataset = full_dataset.take(train_size).batch(mini_batch_size).prefetch(AUTOTUNE).cache()\n",
    "test_dataset = full_dataset.skip(train_size).batch(mini_batch_size).prefetch(AUTOTUNE).cache()\n",
    "\n",
    "# for feat, targ in train_dataset.shuffle(shuffle_buffer):\n",
    "# #         print ('Features: {0}'.format(feat, targ))\n",
    "#     print ('Target train: {1}'.format(feat, targ))\n",
    "    \n",
    "# for feat, targ in test_dataset.shuffle(shuffle_buffer):\n",
    "# #         print ('Features test: {}, Target test: {}'.format(feat, targ))\n",
    "#         print ('Target test: {1}'.format(feat, targ))\n",
    "\n",
    "# print(\"{0} train batches and {1} test batches of {2} mini batch size and {3} steps per epoch\".format(len(train_dataset), \n",
    "#                                                                               len(test_dataset),\n",
    "#                                                                               mini_batch_size,\n",
    "#                                                                                 steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Early stop configuration\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_accuracy', min_delta=1e-3,\n",
    "  patience=200)\n",
    "\n",
    "training_earlystop_callback = EarlyStopping(\n",
    "  monitor='accuracy', min_delta=1e-4,\n",
    "  patience=200)\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        earlystop_callback,\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2e2, min_delta=1e-5),\n",
    "        #tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, train_dataset, test_dataset, name, optimizer=None, max_epochs=1e3):\n",
    "    tf.keras.backend.clear_session()# avoid clutter from old models and layers, especially when memory is limited\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    tf.random.set_seed(seed) # establecemos la semilla para tensorflow\n",
    "    history = model.fit(train_dataset, \n",
    "                        use_multiprocessing=True, \n",
    "                        validation_data=test_dataset, epochs=max_epochs, \n",
    "                        callbacks=get_callbacks(name),\n",
    "                        verbose=0, shuffle=True)\n",
    "    return history\n",
    "\n",
    "# Many models train better if you gradually reduce the learning rate during training. \n",
    "# Use optimizers.schedules to reduce the learning rate over time:\n",
    "def get_optimizer(steps_per_epoch=1, lr=1e-4, multiplier=1e3):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(lr,\n",
    "                                                                 decay_steps=steps_per_epoch*multiplier,\n",
    "                                                                 decay_rate=1,\n",
    "                                                                 staircase=False)\n",
    "    return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/06/14 19:15:53 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "size_histories = {}\n",
    "run = 'lstm/tiny'\n",
    "mlflow.tensorflow.autolog(every_n_iter=1)\n",
    "do = 0.20\n",
    "units = 128\n",
    "\n",
    "mlflow.start_run(run_name=run)\n",
    "mlflow.log_param(\"seed\", seed)\n",
    "mlflow.log_param(\"drop_out\", do)\n",
    "mlflow.log_param(\"mini_batch_size\", mini_batch_size)\n",
    "mlflow.log_param(\"train_split\", train_split)\n",
    "mlflow.log_param(\"test_split\", test_split)\n",
    "mlflow.log_param(\"lstm_units\", units)\n",
    "mlflow.log_param(\"features\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units, activation=tf.nn.tanh, return_sequences=True, input_shape=(n_timesteps,n_features)))\n",
    "model.add(tf.keras.layers.LSTM(units, activation=tf.nn.tanh, return_sequences=True ))\n",
    "model.add(tf.keras.layers.Droput(do))\n",
    "model.add(tf.keras.layers.LSTM(units, activation=tf.nn.tanh, return_sequences=False ))\n",
    "model.add(tf.keras.layers.Dropout(do))\n",
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, name='hidden_layer'))\n",
    "model.add(tf.keras.layers.Dense(n_outputs, activation=tf.nn.sigmoid, name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 4096, 128)         68096     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4096, 128)         131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 344,467\n",
      "Trainable params: 344,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3034s vs `on_train_batch_end` time: 0.4089s). Check your callbacks.\n",
      "\n",
      "Epoch: 0, accuracy:0.4571,  loss:1.0661,  val_accuracy:0.6667,  val_loss:1.0444,  \n",
      "....................................................................................................\n",
      "Epoch: 100, accuracy:1.0000,  loss:0.0399,  val_accuracy:0.8333,  val_loss:0.5778,  \n",
      "....................................................................................................\n",
      "Epoch: 200, accuracy:0.9714,  loss:0.0438,  val_accuracy:0.8889,  val_loss:0.6473,  \n",
      "......................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqhlgpr6q/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqhlgpr6q/model/data/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################Evaluation###########################\n",
      "train acc: 1.0\n",
      "test acc: 0.8888888955116272\n"
     ]
    }
   ],
   "source": [
    "size_histories[run] = compile_and_fit(model, train_dataset, test_dataset,\n",
    "                                              run, \n",
    "                                              optimizer=get_optimizer(),\n",
    "#                                               optimizer=tf.keras.optimizers.Adam(0.001), \n",
    "#                                               optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=0.9),\n",
    "                                              max_epochs=1000)\n",
    "\n",
    "print(\"\\n#######################Evaluation###########################\")\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('train acc:', max(size_histories[run].history[\"accuracy\"]))\n",
    "print('test acc:', max(size_histories[run].history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
