{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFStore=pd.HDFStore\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow_docs as tfdocs\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from utils import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data parameters\n",
    "num_coefficients=int(config[\"coefficients\"])\n",
    "root_path=config[\"root_path\"]\n",
    "hw_results_path= config[\"hw_results_path\"]\n",
    "\n",
    "# training parameters\n",
    "seed=int(config[\"seed\"]) if \"seed\" in config.keys() else 42\n",
    "\n",
    "dr=float(config[\"dropout\"]) if \"dropout\" in config.keys() else 0.2\n",
    "lr2=float(config[\"lr2\"]) if \"lr2\" in config.keys() else 1e-3\n",
    "lr1=float(config[\"lr1\"]) if \"lr1\" in config.keys() else 1e-4\n",
    "lr=float(config[\"lr\"]) if \"lr\" in config.keys() else 8e-4\n",
    "\n",
    "num_epochs=int(config[\"num_epochs\"]) if \"num_epochs\" in config.keys() else 1000\n",
    "num_features=int(config[\"features\"]) if \"features\" in config.keys() else 4096\n",
    "mini_batch_size=int(config[\"mini_batch_size\"]) if \"mini_batch_size\" in config.keys() else 4\n",
    "\n",
    "main_units=int(config[\"main_units\"]) if \"main_units\" in config.keys() else 64\n",
    "secondary_units=int(config[\"secondary_units\"]) if \"secondary_units\" in config.keys() else 16\n",
    "last_unit=int(config[\"last_unit\"]) if \"last_unit\" in config.keys() else 8\n",
    "lstm_units=int(config[\"lstm_units\"]) if \"lstm_units\" in config.keys() else 64\n",
    "num_classes=int(config[\"num_classes\"]) if \"num_classes\" in config.keys() else 1\n",
    "\n",
    "\n",
    "print_sample=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/elekin/data/results/03-HANDWRITTING/archimedean-17-splits.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/residues/train/features',\n",
       " '/residues/train/labels',\n",
       " '/residues/test/features',\n",
       " '/residues/test/labels']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data from hdf5 file\n",
    "rdo_root_path = path.join(root_path,hw_results_path)\n",
    "h5file = path.join(rdo_root_path, \"archimedean-\")\n",
    "h5filename = h5file + str(num_coefficients) + \"-splits.h5\"\n",
    "print(h5filename)\n",
    "hdf = HDFStore(h5filename)\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and scale timeseries between 0 and 1\n",
    "x_train = hdf[\"/residues/train/features\"].values.astype('float32')\n",
    "y_train = hdf[\"/residues/train/labels\"].values.astype('int8').reshape(-1,1)\n",
    "x_test = hdf[\"/residues/test/features\"].values.astype('float32')\n",
    "y_test = hdf[\"/residues/test/labels\"].values.astype('int8').reshape(-1,1)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 train batches and 5 test batches of 4 mini batch size and 2 steps per epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ((None, 4096), (None, 1)), types: (tf.float32, tf.int8)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).take(len(x_train)).batch(mini_batch_size).prefetch(2).cache()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).take(len(x_test)).batch(mini_batch_size).prefetch(2).cache()\n",
    "steps_per_epoch = round(len(train_dataset)/mini_batch_size)\n",
    "\n",
    "if print_sample:\n",
    "    for feat, targ in test_dataset.take(10):\n",
    "        print ('Features test: {}, Target: {}'.format(feat, targ))\n",
    "\n",
    "    for feat, targ in test_dataset.take(10):\n",
    "        print ('Features train: {}, Target: {}'.format(feat, targ))\n",
    "\n",
    "print(\"{0} train batches and {1} test batches of {2} mini batch size and {3} steps per epoch\".format(len(train_dataset), \n",
    "                                                                              len(test_dataset),\n",
    "                                                                              mini_batch_size,\n",
    "                                                                                steps_per_epoch))\n",
    "test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
